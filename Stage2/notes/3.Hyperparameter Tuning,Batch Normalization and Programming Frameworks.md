1. Hyperparameter Tuning: Tray random values don't use a grid:

   ![image-20240206095307314](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240206095307314.png)

   ![image-20240206095324523](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240206095324523.png)

   ![image-20240206095331274](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240206095331274.png)

2. Using an appropriate scale to pick hyper parameters:

   ![image-20240206101600836](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240206101600836.png)

   Logarithmic Scaling:

   ![image-20240206101656865](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240206101656865.png)

   ![image-20240206101705032](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240206101705032.png)

3. Hyperparameters Tuning in Practice: Pandas vs. Caviar:

   ![image-20240206103119406](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240206103119406.png)

4. Normalizing activations in a network: 要么normalize z要么normalize a，一般normalize z比较多

   ![image-20240206105028506](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240206105028506.png)

   Batch norm 作用：在神经网络中，每一层的输出成为了下一层的输入，这就形成了一个层层传递的过程。如果在这个过程中，数据的分布发生了变化，也就是所谓的“内部协变量偏移”（internal covariate shift），那么模型在学习的时候就会遇到困难。因为每一层都需要不断调整自己，以适应前一层输出数据分布的变化，这就导致了学习过程的不稳定性，特别是在深层网络中，这个问题会更加明显。Batch Norm（批量归一化）就是为了解决这个问题而提出的。它通过对每一层的输入（也就是前一层的输出）进行归一化处理，确保这些输入数据在经过激活函数之前的分布保持稳定。

   ![image-20240206105204354](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240206105204354.png)

   ![image-20240206105455023](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240206105455023.png)

5. Fitting Batch Norm into a Neural Network:

   ![image-20240206124203534](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240206124203534.png)

   No need b in z=wa+b![image-20240206112158458](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240206112158458.png)

6. Why Batch Normalization works

   ![image-20240206125313756](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240206125313756.png)	What is Covariate shift:

   ![image-20240206130155587](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240206130155587.png)

   w h y it can reduce covariate shift:

   ![image-20240206131719362](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240206131719362.png)

7. Batch Norm at test time:

   ![image-20240206135432273](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240206135432273.png)

   

8. Multi-class Classification:

   ![image-20240206143137130](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240206143137130.png)

9. Training a softmax classifier:

   ![image-20240206152751736](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240206152751736.png)

   ![image-20240206152813075](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240206152813075.png)

10. Deep learning framework:

    
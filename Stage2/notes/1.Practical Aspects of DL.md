1. bias and variance:

   In deep learning, bias and variance are two crucial concepts that help us understand the behavior of predictive models in terms of their generalization ability. They are part of the bias-variance tradeoff, a fundamental principle that describes the balance between the error introduced due to the model's simplicity (bias) and the error introduced due to its complexity (variance).

   ### Bias

   Bias refers to the error due to overly simplistic assumptions in the learning algorithm. High bias can cause the model to miss the relevant relations between features and target outputs (underfitting), meaning the model is not complex enough to capture the underlying structure of the data. In deep learning, a model with high bias might be too shallow, with too few layers or neurons, and thus unable to learn the complexity of the data.

   ### Variance

   Variance refers to the error due to too much complexity in the learning algorithm. High variance can cause the model to model the random noise in the training data (overfitting), rather than the intended outputs. This means the model is too sensitive to the fluctuations in the training data, and it might not generalize well to unseen data. In deep learning, a model with high variance might have many layers or neurons, making it capable of fitting a wide variety of functions, including noise in the training data.

   ### Bias-Variance Tradeoff

   The bias-variance tradeoff is the point where we are adding just enough complexity to the model so that it learns well from the training data, without starting to learn from the noise. The goal is to find a good balance between bias and variance, minimizing the total error. This often involves tuning the architecture of the neural network (such as the number of layers and neurons), as well as regularization techniques to prevent overfitting, and using techniques like cross-validation to estimate model performance on unseen data.

   ### Managing Bias and Variance in Deep Learning

   To manage bias and variance in deep learning, practitioners often experiment with the model architecture, add dropout layers or batch normalization, use regularization techniques (like L1/L2 regularization), and ensure they have sufficient data. Using a more complex model can reduce bias but might increase variance; on the other hand, simplifying the model can reduce variance but increase bias. The key is to find a balance that minimizes the total error on unseen data.

   ![image-20240130095841700](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240130095841700.png)

2. Bayes optimal error:

   Bayes optimal error, also known as the Bayes error rate, represents the lowest possible error that any classifier can achieve when predicting the true class. This error rate serves as a theoretical minimum error and is determined by the intrinsic noise within the data distribution itself. It's important to understand that the Bayes optimal error does not stem from the limitations of a particular model or learning algorithm but rather from the complexity and overlap of the underlying data distributions.

   Bias和variance是相对于bayes optimal error来说,如果

3. Evaluate the bias and variance based on Train and test set error:这是相对于otpimal bayes eror约等于0

   ![image-20240130101035734](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240130101035734.png)

4. How to solve Bias and variance?

   1. High bias? 1. Bigger network 2. Training longer 3. Different NN architecture
   2. High variance? 1. More data 2. Regularization 3.  Different NN architecture

5. Regularization:

   ![image-20240130113028812](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240130113028812.png)

   for a neural network: It's called Frobenius norm of matrix instead of L2 norm of matrix here. The L2 regularization is called weight decay 因为最后一行化简后的结果相当于给w乘了个小于1的系数

   ![image-20240130113743477](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240130113743477.png)

6. How does regularization prevent overfitting?

   ![image-20240130124552206](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240130124552206.png)但lambda很大的时候w趋向于0，那么中间有些神经元就约等于被丢掉了就可以防止模型太复杂导致的过拟合big variance问题

   ![image-20240130124704546](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240130124704546.png)

   还有这种解释就是说通过缩小w将z的范围缩小到红色线性区域，这样就可以将本来过拟合的非线性boundary扭正成线性的

7. Dropout Regularization:

   ![image-20240131135704707](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240131135704707.png)

   The image you've uploaded appears to be a handwritten note about implementing dropout in neural networks, specifically a version known as "inverted dropout." Here's how the process described in the image works:

   1. `keep_prob = 0.8`: This indicates that during training, each neuron (or unit) in the layer will be kept with a probability of 80%, and dropped with a probability of 20%. This is a common technique used to prevent overfitting in neural networks.

   2. `D3 = np.random.rand(a3.shape[0], a3.shape[1]) < keep_prob`: This line of code creates a dropout matrix `D3` the same size as the activation matrix `a3`. `np.random.rand` generates random numbers between 0 and 1. The comparison `< keep_prob` turns these into a boolean matrix where `True` has a value of 1 and represents that the neuron will be kept, and `False` has a value of 0 and means the neuron will be dropped.

   3. `a3 = np.multiply(a3, D3)`: This applies the dropout matrix to the activation matrix `a3`. The `np.multiply` function performs element-wise multiplication. Only the activations where `D3` is 1 (True) are kept; the rest are set to 0.

   4. `a3 /= keep_prob`: To compensate for the reduced number of active neurons during training, the activations are scaled up by dividing by `keep_prob`. This step is crucial as it maintains the expected value of the activations the same as if dropout was not applied, which is an aspect of "inverted dropout". Without this scaling, the network will tend to perform differently during training and inference (when dropout is not applied).

   The illustration also includes a note about the network with 50 units, where ~10 units are shut off, suggesting that this is an example calculation based on the keep probability. The network during testing does not apply dropout, hence the term "test" is written to indicate that this scaling is not performed at test time.

8. why don't do  dropout during prediction:

   Dropout is a regularization technique used during the training of neural networks to prevent overfitting. During training, dropout randomly deactivates a subset of neurons (i.e., units in a neural network layer) with a certain probability (like flipping coins to decide which neurons to drop). This forces the network to learn more robust features that are useful in conjunction with many different random subsets of the other neurons.

   However, at prediction (or test) time, dropout is not used, and here's why:

   1. **Removing Randomness**: At prediction time, you want the output of the network to be deterministic. Using dropout during prediction would introduce randomness in the output, which is undesirable. When you make a prediction, you typically want the same output for the same input every time.

   2. **Maintaining Expected Values**: During training, when you use inverted dropout (which involves scaling the activations by the inverse of the dropout probability), you ensure that the expected value of the output remains the same as it would be without dropout. This means that the scaling done during training compensates for the fact that some neurons are dropped. If you were to use dropout at test time without this scaling, the expected values of the activations would be different from what the network learned during training.

   3. **Full Capacity of the Network**: At test time, you want to use the full capacity of the network. The network during training learns to make accurate predictions with a reduced network (because of dropout). So, at test time, when dropout is not applied, the network can use all the neurons, which theoretically could lead to better performance since it's leveraging all available resources.

   4. **Computational Efficiency**: As you mentioned, one could, in theory, run the prediction process many times with dropout and average the results to get an estimate that simulates the training conditions. However, this is computationally expensive and unnecessary because the scaling during training (inverted dropout) already ensures that the activations at test time will be on the correct scale.

   To summarize, during training, dropout helps to simulate a large number of thinned networks with shared weights, which leads to a more robust model. At test time, you want to use the full, learned network to make the most accurate predictions possible. The key is that the scaling during training (due to inverted dropout) ensures that the network's learned weights are appropriate for the full network at test time.

9. Why Dropout work:

   Dropout works as a regularization technique in neural networks for several reasons:

   1. **Prevents Co-Adaptation of Neurons**: By randomly dropping units (neurons) during training, dropout prevents neurons from becoming overly dependent on the specific contributions of other neurons. This means that each neuron must learn to operate well in a variety of different network contexts and cannot rely on any single input too heavily.

   2. **Encourages Redundancy**: Since neurons cannot rely on the presence of others, they must learn more robust features that are useful in conjunction with many different random subsets of other neurons. This encourages the network to create multiple pathways for the information, which can provide redundancy in the representation.

   3. **Effectively Ensembles Many Networks**: During each training iteration, a different "thinned" network is used, which can be seen as sampling from the space of all possible subnetworks of the original architecture. The final model can be thought of as an ensemble of all these thinned networks, which often leads to better generalization.

   4. **Mimics L2 Regularization (to some extent)**: It has been shown that dropout can act as an adaptive form of L2 regularization. The dropout procedure tends to spread out the weights of the neurons, preventing any single weight from having too much influence on the result. This spreading out of weights has an effect similar to L2 regularization, which penalizes the square of the weights.

   5. **Adaptive Regularization**: The regularization effect of dropout is adaptive—it affects each weight differently, depending on the scale of the activations that the weight is multiplied with. This means that the dropout not only regularizes the network but does so in a way that is sensitive to the network's behavior, which is not the case with traditional regularization methods like L2, where every weight is regularized equally.

   6. **Flexibility in Application**: Dropout can be applied differently across layers. For layers where overfitting is a bigger concern (often layers with more parameters), you can set a lower keep probability (higher dropout rate), effectively increasing the regularization. For layers where overfitting is less of a concern, a higher keep probability can be used, or even set to 1.0 (no dropout).

   7. **Computationally Simple**: Dropout is easy to implement and computationally cheap compared to other regularization methods. It doesn't require significant changes to the underlying network architecture or learning algorithm.

   However, dropout does have some downsides, such as:

   1. **No Well-Defined Cost Function**: With dropout, the cost function is not well-defined because the network architecture is different on each iteration (due to different neurons being dropped). This makes it harder to monitor and ensure the cost function is decreasing over time.

   2. **Increased Complexity in Hyperparameter Tuning**: The use of dropout adds another hyperparameter to tune (the dropout rate), which can increase the complexity of the model selection process.

   In summary, dropout is an effective regularization technique because it encourages the neural network to become less sensitive to the specific weights of neurons, leading to a more generalized model that performs better on unseen data. It does this by introducing randomness during training, forcing neurons to learn to work with different subsets of features, and preventing them from relying too heavily on any one feature.

10. Other regularization methods:

   11. Data augmentation

       ![image-20240131150617581](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240131150617581.png)

       Data augmentation is a strategy used to increase the diversity of a training dataset without actually collecting new data. This is achieved by applying various transformations to the existing data to create modified versions of the data points. Here's a summary of how it works and its benefits:

       **Process of Data Augmentation:**

       1. **Image Transformations**: For image data, common augmentations include flipping the image horizontally or vertically, rotating, zooming in or out, cropping, changing the brightness or contrast, and adding noise.

       2. **Audio Transformations**: For audio data, you might add noise, change the pitch, adjust the speed, or apply other filters.

       3. **Text Transformations**: For text, augmentations can include synonym replacement, random insertion, swapping words, or deleting words.

       **Benefits of Data Augmentation:**

       1. **Reduces Overfitting**: By creating more varied training examples, the model becomes less likely to learn noise and specific details of the training data, thereby improving its generalization capabilities.

       2. **Increases Dataset Size**: Augmentation artificially inflates the dataset size, which can be particularly beneficial when the amount of available data is limited.

       3. **Incorporates Invariance**: By teaching the model that certain transformations do not change the underlying class or meaning, the model can learn to be invariant to these transformations. For example, a cat is still a cat whether the image is mirrored or not.

       4. **Improves Model Robustness**: Models trained on augmented data can become more robust to variations and noise in real-world data, which they may encounter when deployed.

       5. **Cost-Effective**: Data augmentation is a cost-effective way to enhance a dataset since it does not require the time and resources that would be needed to collect and label new data.

       In summary, data augmentation is a valuable technique in machine learning for enhancing the size and quality of training datasets, leading to more robust and generalized models. It's particularly useful when the original dataset is small or lacks diversity.

   12. Early stopping

       ![image-20240131150821963](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240131150821963.png)(防止test set error上去)

       Early stopping is a regularization technique that is used to prevent overfitting in neural networks. Here's a summary of its advantages and downsides:

       **Advantages of Early Stopping:**

       1. **Computational Efficiency**: Early stopping saves computational resources since it halts training before the maximum number of iterations is reached.
       2. **Prevents Overfitting**: By halting the training process when the validation error starts to increase, early stopping prevents the model from learning the noise in the training data, which can lead to overfitting.
       3. **Quick Experimentation**: It allows for faster experimentation with different models because you do not have to train to full convergence each time.
       4. **No Need for an Explicit Regularization Hyperparameter**: Unlike methods like L2 regularization that require tuning a hyperparameter (lambda), early stopping inherently provides a form of regularization without the need to search for the best hyperparameter value.

       **Downsides of Early Stopping:**

       1. **Coupling of Optimization and Regularization**: Early stopping combines the tasks of fitting the data and regularizing the model, which can complicate the optimization process. It might make it more difficult to discern whether improvements are due to better optimization or better regularization.
       2. **Hyperparameter Choice**: While it avoids the need to choose a regularization strength, you still have to decide on other hyperparameters such as when to stop training. The decision of when to stop becomes a hyperparameter itself, which may require validation.
       3. **Orthogonalization**: Early stopping violates the principle of orthogonalization, where you ideally have separate tools for optimizing the cost function and for preventing overfitting. This principle makes it easier to understand and control the behavior of the machine learning system.
       4. **Potentially Suboptimal Utilization of Data**: By stopping early, you may not fully utilize all the information that the data can provide for the learning process.

       To summarize, early stopping can be a useful technique for reducing overfitting, offering a simple way to regularize without extensive hyperparameter tuning. However, it can complicate the training process by combining optimization and regularization into a single step, making it harder to manage the machine learning workflow.

   13. Normalization inputs:

       ![image-20240206112131041](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240206112131041.png)

       ![image-20240131154136566](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240131154136566.png)

       ![image-20240131154226986](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240131154226986.png)

   14. Gradient Exploding or Vanishing:

       ![image-20240201112048511](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240201112048511.png)

       ![image-20240201112035653](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240201112035653.png)

       梯度爆炸消失产生的原因![image-20240206134820235](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240206134820235.png)

       

   15. Weight initialization to solve vanishing/exploding gradient:

       ![image-20240201130829657](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240201130829657.png)

       ![image-20240201131141304](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240201131141304.png)

       对于relu方差设成2/n, 对于tanh方差设成右上角

   16. Gradient checking to debug:

       ![image-20240202095046261](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240202095046261.png)

   17. Gradient checking implementation notes:

       ![image-20240202095805807](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240202095805807.png)

       

       

# 如何在大数据集下加速神经网络训练的算法：

1. Speed up1: Minibatch gradient descent:

   Vectorization 可以加快训练速度但是如果当instance number很大的时候训练速度仍然会很慢，所以minibatch 来分割

   ![image-20240203092838233](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240203092838233.png)

   正常的batch 1 epoch（1 pass through training set）allow you to take 1 gradient descent step, 如果number of instance m=5000000,每个mini batch是1000，那么如下的mini batch gradient descent每个epoch takes 5000 gradient descent step ![image-20240203092938603](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240203092938603.png)

   因为cost J的计算是针对每个mini batch的，每个mini batch训练集都不一样所以每次计算的J也有可能不同。![image-20240203094801794](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240203094801794.png)

   choose your mini-batch size:

   蓝色和紫色是极端情况，绿色是最好的情况，紫色这种如果mini batch size太小就不能很好利用vecotrization加速，蓝色的话时间太长了

   ![image-20240203103856109](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240203103856109.png)

   一般选择2的指数次![image-20240203104434823](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240203104434823.png)

   

   

   

2. Exponentially Weighted Averages(EWA):

   ![image-20240203111833415](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240203111833415.png)

   ![image-20240206100955759](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240206100955759.png)

   ![image-20240203111912988](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240203111912988.png)

   

3. Bias correction:用来correct初始阶段的Exponentially Weighted Averages(可有可无)

   ![image-20240203113447536](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240203113447536.png)

   Bias correction可以前几步用后面就可以用标准的EWA了，也可以按如下方式整合

   ![image-20240203113902319](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240203113902319.png)

   刚开始是紫色的这条线，估值偏小所以要bias correction，然后变成绿色的线：

   ![image-20240203113948954](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240203113948954.png)

   ![image-20240203115143169](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240203115143169.png)

4. Speed up2: Gradient Descent with Momentum:

   ![image-20240203151801558](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240203151801558.png)

   ![image-20240203151932209](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240203151932209.png)

   ![image-20240203152216073](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240203152216073.png)

5. Speed up3: RMSprop(root mean square prop):  it adjusts the learning rate for each parameter, making it smaller for parameters with large gradients and larger for parameters with small gradients.

   ![image-20240203160331915](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240203160331915.png)

   ![image-20240203160350617](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240203160350617.png)

   

6. Speed Up4: Adam: Adaptive momentum estimation（结合了M omentum和RMSprop）

   ![image-20240203163136080](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240203163136080.png)

   ![image-20240203164324727](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240203164324727.png)

7. Learning Rate decay:

   ![image-20240203165710695](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240203165710695.png)

   example:

   ![image-20240203165845025](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240203165845025.png)

8. The challenge for optimization:

   ![image-20240203171841630](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240203171841630.png)

   ![image-20240203171903097](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240203171903097.png)

   因为如果有20000local optima的话，那要么是local min或者max，概率为1/2，如果都是min的话概率如下基本不可能所以就会导致不少max，这样就会形成鞍点：

   ![image-20240203172344063](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240203172344063.png)

   

   
# Shallow Neural Network

1. what is a neural network:

   ![image-20240123093529447](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240123093529447.png)

   每个神经元就是一对z和a

2. Neural Network Representation:

   ![image-20240123095034151](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240123095034151.png)

   input layer不算一层，Hidden layer才是第一层所以这个是2 layer NN

3. Vectorizing across multiple examples:横向是total example, 纵向是hidden unit

   ![image-20240123121445676](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240123121445676.png)

4. Pros and cons of activation functions:

   Choosing the right activation function for a neural network depends on several factors, including the type of problem you're solving, the characteristics of the data, and the specific requirements of the model. Here's a guide on when to use each of the commonly used activation functions:

   1. **Sigmoid (Logistic) Function**
      - Use for:
        - Problems where you need probabilities as output, since the output is in the range (0,1).
        - Output layers of binary classification problems.
      - Avoid for:
        - Hidden layers in deep networks due to the vanishing gradient problem.

   2. **Hyperbolic Tangent (tanh) Function**
      - Use for:
        - Hidden layers in neural networks, especially if the data is centered around zero.
        - Problems where you need outputs in the range (-1, 1).
      - Avoid for:
        - Very deep networks, as it also suffers from the vanishing gradient problem.

   3. **Rectified Linear Unit (ReLU) Function**
      - Use for:
        - Most deep learning networks, especially in hidden layers, due to its computational efficiency and effectiveness in mitigating the vanishing gradient problem.
        - Convolutional Neural Networks (CNNs) and Deep Feedforward Neural Networks.
      - Avoid for:
        - Networks where dead neurons become an issue, unless you have a mechanism to deal with this.

   4. **Leaky ReLU**
      - Use for:
        - Situations where you suspect that dying ReLU could be a problem.
        - Deep networks, as it allows for a small gradient when the unit is inactive.

   5. **Parametric ReLU (PReLU)**
      - Use for:
        - Networks where you want to give the model the flexibility to learn the appropriate slope of the negative part.
      - Avoid for:
        - Smaller datasets or simpler networks, where the added complexity might lead to overfitting.

   6. **Exponential Linear Unit (ELU)**
      - Use for:
        - Networks where you want improved learning characteristics compared to ReLU or Leaky ReLU.
        - Problems where reducing the vanishing gradient effect is important.
      - Avoid for:
        - Situations where computational efficiency is a higher priority, as ELUs are more computationally intensive.

   7. **Softmax Function**
      - Use for:
        - The output layer of multi-class classification problems, where you need probabilities for multiple classes.
      - Avoid for:
        - Hidden layers, as its primary purpose is for classification output.

   In practice, the choice of an activation function can also be influenced by empirical results; often, the best way to choose is to experiment with a few different functions and see which one performs best for your specific dataset and problem. Additionally, advancements in deep learning might lead to the development of new activation functions that could be more suited to certain types of problems.

   ![image-20240123140637262](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240123140637262.png)

   

   如果要求输出是01binary classifcation那么output layer用sigmoid, Relu是最常用的

5. Why do you need Non-Linear Activation Functions for neural network?

   Non-linear activation functions are crucial in neural networks for several reasons:

   1. **Introducing Non-linearity**: The primary reason is to introduce non-linearity into the network. Without non-linearity, a neural network, regardless of how many layers it has, would behave like a linear model. This limits its ability to model complex data relationships. Non-linear functions allow the network to capture and learn more complex patterns in the data.

   2. **Deep Learning Capabilities**: The introduction of non-linearity allows for deep learning. With non-linear activation functions, deep neural networks can learn features and patterns at various levels of abstraction. As we move deeper into the network, the complexity and abstraction of what the network can learn and represent increases significantly.

   3. **Backpropagation and Gradient Descent**: Activation functions that are differentiable play a key role in enabling backpropagation. In backpropagation, gradients are computed to update the weights of the network, and for this calculation, the derivative of the activation function is necessary. Without a non-linear, differentiable function, this wouldn't be possible.

   4. **Control of Output Range**: Some activation functions like the sigmoid or hyperbolic tangent function squash the output into a bounded range. This is particularly useful in cases like binary classification where we want the output to represent a probability.

   5. **Biological Plausibility**: While not a primary reason for their use in most applications, non-linear activation functions are often seen as a way to mimic the firing patterns of biological neurons, which are not linear in nature.

   Popular non-linear activation functions include ReLU (Rectified Linear Unit), Sigmoid, and Tanh (Hyperbolic Tangent). Each has its own characteristics and is suitable for different types of neural network layers and architectures.

6. Derivatives of activation functions:

   ![image-20240123152501210](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240123152501210.png)

   ![image-20240123152522489](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240123152522489.png)

   ![image-20240123152539925](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240123152539925.png)

7. Neural network gradient:

   ![image-20240124135912814](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240124135912814.png)

8. Random Initialization:

   If we initialize the w to be 0, then it will have symmetry breaking problem that is all function calculates the same value

   Why to choose 0.01 instead of 100: 如果w太大就会导致z太大，这样z就会在函数两端这样梯度变化就会很慢

   ![image-20240124141526268](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240124141526268.png)

   

   
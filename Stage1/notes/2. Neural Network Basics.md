# Neural Network Basics

1. how to convert the image to vector:

   ![image-20240118113433653](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240118113433653.png)

   ![image-20240118102840735](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240118102840735.png)

   当有m个training instance时，将m个instance横向堆叠所以feature vector X的dimension就是nx*m, Y也横向堆叠，Y.shape=(1,m)

2. logistic regression:

   <img src="/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240118113658257.png" alt="image-20240118113658257" style="zoom:50%;" />

3. The loss function computes the error for a single training example; the cost function is the average of the loss functions of the entire training set.

   ![image-20240118115309094](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240118115309094.png)

4. Gradient Descent：w和b更新方式，这里alpha是learning rate也就是步长![image-20240118142454134](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240118142454134.png)

5. computation graph: 从左到右是forward propagation, 从右到左是backwards propagation,

   ![image-20240118154058177](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240118154058177.png)

   In the context of neural networks and machine learning, forward propagation and backward propagation are two fundamental processes that occur during the training of a neural network. They are essential for understanding how neural networks learn from data.

   1. **Forward Propagation**:
       - **Definition**: Forward propagation is the process of moving the input data through the neural network to generate an output. This is essentially how the neural network makes predictions.
       - **Process**: During forward propagation, the input data is fed into the network. It then passes through the layers of the network, where each layer applies specific weights and biases, often followed by a non-linear activation function. The result is the predicted output at the end.
       - **Purpose**: The main purpose of forward propagation is to compute the output based on the current state of the network’s weights and biases.

   2. **Backward Propagation**:
       - **Definition**: Backward propagation, often called backpropagation, is the process of tuning the weights and biases of the neural network based on the error of the prediction. It is a key part of the training process.
       - **Process**: In backward propagation, the error (or loss) is calculated by comparing the predicted output of the network with the actual target values. This error is then propagated back through the network, starting from the output layer to the input layer. During this process, the weights and biases are adjusted in order to minimize the error.
       - **Purpose**: The main purpose of backward propagation is to optimize the weights and biases so that the network’s predictions become more accurate over time.

   In a typical training cycle of a neural network, forward propagation is used to calculate the output for a batch of data, and then backward propagation is used to update the network’s weights and biases based on the error of that output. This cycle is repeated many times with different batches of data until the network becomes proficient at making accurate predictions.

6. logistic regression derivatives for one example:

   ![image-20240118164626526](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240118164626526.png)

   ![image-20240118164629147](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240118164629147.png)

   反向传播链式求导得到L对于w1,w2,b的偏导,然后根据右下角更新这3个值

7. logistic regression derivatives for m example: 对6中取到的单个的derivative求和取平均

   ![image-20240119100833990](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240119100833990.png)

   ![image-20240119100843255](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240119100843255.png)

   这里需要2个loop，一个loop遍历m个instance，另一个loop遍历n个参数，但是嵌套的loop太耗时了，所以用vectorization

# Python and Vectorization

1. python and vectorization:

```python
import time
import numpy as np

a=np.random.rand(1000000)
b=np.random.rand(1000000)
tic=time.time()
c=np.dot(a,b)
toc=time.time()

print(c)
print("Vectorized version:"+str(1000*(toc-tic))+"ms")

c=0
tic=time.time()
for i in range(1000000):
    c+=a[i]*b[i]
toc=time.time()
print(c)
print("For loop:"+str(1000*(toc-tic))+"ms")
```

result:

![image-20240120095305857](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240120095305857.png)

2. set dw as a vector:

   ![image-20240120095636810](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240120095636810.png)

3. vectorizing logistic regression:

   ![image-20240120100659987](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240120100659987.png)

   可以用z=np.dot(w.T,x)+b来直接算z来避免for loop

4. Implementing logistic regression with complete vectorization(code on the right), but we can't get rid of the iteration number of gradient descent:

   ![image-20240120103125394](/Users/nanzhenghan/Library/Application Support/typora-user-images/image-20240120103125394.png)

   

   

